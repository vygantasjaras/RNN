{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Prof. Dr. Georgios K. Ouzounis\n",
    "<br/> email: georgios.ouzounis@go.kauko.lt\n",
    "\n",
    "<br/> Student: Vygantas Jaras\n",
    "<br/> email: vygantas.ja9484@go.kauko.lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Data loading and pre-processing\n",
    "2. Building the RNN\n",
    "3. Train and deploy the RNN\n",
    "4. Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.sapientrade.com/images/2017/02/14/AI-Machine-Learning-Trading-Benefits.jpg\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 10-year history of the Intel Stock prices predict the stock values for the period of the recent most month that are not included in the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data-sets\n",
    "\n",
    "The data-sets are two comma-separated values files (CSV) and contain a data table of 2538 records for training and a table of 22 records for testing.\n",
    "\n",
    "These data-sets were downloaded from yahoo finances\n",
    "\n",
    "**Intel_Stock_Price_Test.csv** and **Intel_Stock_Price_Train.csv**\n",
    "\n",
    "\n",
    "\n",
    "Open a terminal and use the wget command to get it of the selected location. Example:\n",
    "\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/vygantasjaras/RNN/main/Intel_Stock_Price_Test.csv\n",
    "\n",
    "wget https://raw.githubusercontent.com/vygantasjaras/RNN/main/Intel_Stock_Price_Train.csv \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We need 3 main libraries:\n",
    "\n",
    "- [Numpy](http://www.numpy.org): it is the fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object that can be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined.\n",
    "- [matplotlib](https://matplotlib.org):  it is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- [pandas](https://pandas.pydata.org): is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description: the open high, low and close values of the Google Stock from 2012 to 2016. [Relevant code here](https://github.com/pdway53/Predict_Google_Stock_Price_RNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "# load the file contents \n",
    "dataset_train = pd.read_csv('Intel_Stock_Price_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-07-31</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>13.682571</td>\n",
       "      <td>47557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-08-03</td>\n",
       "      <td>19.480000</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>19.180000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>13.767866</td>\n",
       "      <td>44330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-04</td>\n",
       "      <td>19.209999</td>\n",
       "      <td>19.379999</td>\n",
       "      <td>19.150000</td>\n",
       "      <td>19.320000</td>\n",
       "      <td>13.732331</td>\n",
       "      <td>46880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-08-05</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>13.503215</td>\n",
       "      <td>60035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>18.930000</td>\n",
       "      <td>19.180000</td>\n",
       "      <td>18.559999</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>13.388664</td>\n",
       "      <td>66862100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2009-07-31  19.340000  19.559999  19.250000  19.250000  13.682571  47557600\n",
       "1  2009-08-03  19.480000  19.559999  19.180000  19.370001  13.767866  44330800\n",
       "2  2009-08-04  19.209999  19.379999  19.150000  19.320000  13.732331  46880800\n",
       "3  2009-08-05  19.270000  19.299999  18.860001  18.860001  13.503215  60035000\n",
       "4  2009-08-06  18.930000  19.180000  18.559999  18.700001  13.388664  66862100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subtable of relevant entries (open values)\n",
    "# The .values makes this vector a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.34    ],\n",
       "       [19.48    ],\n",
       "       [19.209999],\n",
       "       ...,\n",
       "       [52.689999],\n",
       "       [51.529999],\n",
       "       [51.709999]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# import the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0356011 ],\n",
       "       [0.0390149 ],\n",
       "       [0.03243111],\n",
       "       ...,\n",
       "       [0.84881736],\n",
       "       [0.82053158],\n",
       "       [0.82492075]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 90 timesteps and 1 output\n",
    "\n",
    "# the 90 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2516, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start from day 90 because that is the first instance allowing us to \n",
    "# go back 90 days\n",
    "for i in range(90, 2516): \n",
    "    # 0 is the column ID, the only column in this case.    \n",
    "    # put the last 90 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0356011 , 0.0390149 , 0.03243111, ..., 0.0458425 , 0.05632777,\n",
       "        0.05681541],\n",
       "       [0.0390149 , 0.03243111, 0.0338942 , ..., 0.05632777, 0.05681541,\n",
       "        0.05779081],\n",
       "       [0.03243111, 0.0338942 , 0.02560354, ..., 0.05681541, 0.05779081,\n",
       "        0.05145089],\n",
       "       ...,\n",
       "       [0.89222146, 0.8822239 , 0.87881002, ..., 0.83199222, 0.83150454,\n",
       "        0.84442816],\n",
       "       [0.8822239 , 0.87881002, 0.89051458, ..., 0.83150454, 0.84442816,\n",
       "        0.84881736],\n",
       "       [0.87881002, 0.89051458, 0.85564501, ..., 0.84442816, 0.84881736,\n",
       "        0.82053158]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "We need to add a new matrix dimension to accommodate the indicator (predictor). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to add the stock value of somebody else together with the the past 60 days of Google, we need to change the length of the 3 dimension to  2.  RNN training tables are 3D!!! Read: [Reshaping NumPy Array | Numpy Array Reshape Examples](https://backtobazics.com/python/python-reshaping-numpy-array-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Layer\n",
    "\n",
    "We first add an object of the LSTM class! \n",
    "\n",
    "- The first argument is the number of units or LSTM memory cells. Include many neurons to address the high dimensionality of the problem; say 50 neurons! \n",
    "- Second arg: return sequences = true; stacked LSTM !\n",
    "- Third arg: input 3D shape: observations vs time steps vs number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%), \n",
    "# i.e. 50 units * 20% = 10 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n",
    "\n",
    "We donâ€™t need to add the shape of the layer again because it is recognized automatically from the number of input units.\n",
    "\n",
    "The last layer does not return a sequence but connected directly to a fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "76/76 [==============================] - 15s 197ms/step - loss: 0.0120\n",
      "Epoch 2/100\n",
      "76/76 [==============================] - 15s 191ms/step - loss: 0.0035\n",
      "Epoch 3/100\n",
      "76/76 [==============================] - 14s 189ms/step - loss: 0.0034\n",
      "Epoch 4/100\n",
      "76/76 [==============================] - 14s 187ms/step - loss: 0.0034\n",
      "Epoch 5/100\n",
      "76/76 [==============================] - 14s 187ms/step - loss: 0.0030\n",
      "Epoch 6/100\n",
      "76/76 [==============================] - 17s 228ms/step - loss: 0.0028\n",
      "Epoch 7/100\n",
      "76/76 [==============================] - 17s 222ms/step - loss: 0.0028\n",
      "Epoch 8/100\n",
      "76/76 [==============================] - 16s 214ms/step - loss: 0.0022\n",
      "Epoch 9/100\n",
      "76/76 [==============================] - 17s 221ms/step - loss: 0.0024\n",
      "Epoch 10/100\n",
      "76/76 [==============================] - 16s 213ms/step - loss: 0.0023\n",
      "Epoch 11/100\n",
      "76/76 [==============================] - 16s 213ms/step - loss: 0.0023\n",
      "Epoch 12/100\n",
      "76/76 [==============================] - 16s 212ms/step - loss: 0.0020\n",
      "Epoch 13/100\n",
      "76/76 [==============================] - 17s 225ms/step - loss: 0.0019\n",
      "Epoch 14/100\n",
      "76/76 [==============================] - 17s 228ms/step - loss: 0.0019\n",
      "Epoch 15/100\n",
      "76/76 [==============================] - 16s 214ms/step - loss: 0.0019\n",
      "Epoch 16/100\n",
      "76/76 [==============================] - 17s 225ms/step - loss: 0.0017\n",
      "Epoch 17/100\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 0.0017\n",
      "Epoch 18/100\n",
      "76/76 [==============================] - 18s 236ms/step - loss: 0.0016\n",
      "Epoch 19/100\n",
      "76/76 [==============================] - 15s 193ms/step - loss: 0.0016\n",
      "Epoch 20/100\n",
      "76/76 [==============================] - 14s 191ms/step - loss: 0.0017\n",
      "Epoch 21/100\n",
      "76/76 [==============================] - 14s 185ms/step - loss: 0.0016\n",
      "Epoch 22/100\n",
      "76/76 [==============================] - 13s 177ms/step - loss: 0.0016\n",
      "Epoch 23/100\n",
      "76/76 [==============================] - 14s 181ms/step - loss: 0.0015\n",
      "Epoch 24/100\n",
      "76/76 [==============================] - 14s 187ms/step - loss: 0.0014\n",
      "Epoch 25/100\n",
      "76/76 [==============================] - 15s 204ms/step - loss: 0.0014\n",
      "Epoch 26/100\n",
      "76/76 [==============================] - 14s 187ms/step - loss: 0.0014\n",
      "Epoch 27/100\n",
      "76/76 [==============================] - 14s 186ms/step - loss: 0.0014\n",
      "Epoch 28/100\n",
      "76/76 [==============================] - 14s 190ms/step - loss: 0.0014\n",
      "Epoch 29/100\n",
      "76/76 [==============================] - 16s 214ms/step - loss: 0.0013\n",
      "Epoch 30/100\n",
      "76/76 [==============================] - 16s 211ms/step - loss: 0.0012\n",
      "Epoch 31/100\n",
      "76/76 [==============================] - 15s 196ms/step - loss: 0.0012\n",
      "Epoch 32/100\n",
      "76/76 [==============================] - 15s 196ms/step - loss: 0.0012\n",
      "Epoch 33/100\n",
      "76/76 [==============================] - 15s 202ms/step - loss: 0.0013\n",
      "Epoch 34/100\n",
      "76/76 [==============================] - 15s 200ms/step - loss: 0.0012\n",
      "Epoch 35/100\n",
      "76/76 [==============================] - 14s 185ms/step - loss: 0.0013\n",
      "Epoch 36/100\n",
      "76/76 [==============================] - 13s 176ms/step - loss: 0.0011\n",
      "Epoch 37/100\n",
      "76/76 [==============================] - 13s 175ms/step - loss: 0.0012\n",
      "Epoch 38/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.0010\n",
      "Epoch 39/100\n",
      "76/76 [==============================] - 13s 176ms/step - loss: 0.0011\n",
      "Epoch 40/100\n",
      "76/76 [==============================] - 13s 176ms/step - loss: 0.0014\n",
      "Epoch 41/100\n",
      "76/76 [==============================] - 13s 175ms/step - loss: 0.0012\n",
      "Epoch 42/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.0011\n",
      "Epoch 43/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.0011\n",
      "Epoch 44/100\n",
      "76/76 [==============================] - 14s 178ms/step - loss: 0.0011\n",
      "Epoch 45/100\n",
      "76/76 [==============================] - 13s 175ms/step - loss: 0.0011\n",
      "Epoch 46/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 9.6856e-04\n",
      "Epoch 47/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.0010\n",
      "Epoch 48/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.0012\n",
      "Epoch 49/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.0012\n",
      "Epoch 50/100\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 9.2393e-04\n",
      "Epoch 51/100\n",
      "76/76 [==============================] - 13s 173ms/step - loss: 0.0010\n",
      "Epoch 52/100\n",
      "76/76 [==============================] - 13s 175ms/step - loss: 8.5225e-04\n",
      "Epoch 53/100\n",
      "76/76 [==============================] - 13s 170ms/step - loss: 9.7194e-04\n",
      "Epoch 54/100\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 9.4718e-04\n",
      "Epoch 55/100\n",
      "76/76 [==============================] - 13s 173ms/step - loss: 0.0011\n",
      "Epoch 56/100\n",
      "76/76 [==============================] - 13s 171ms/step - loss: 9.2487e-04\n",
      "Epoch 57/100\n",
      "76/76 [==============================] - 13s 170ms/step - loss: 0.0011\n",
      "Epoch 58/100\n",
      "76/76 [==============================] - 13s 172ms/step - loss: 9.4267e-04\n",
      "Epoch 59/100\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 9.0605e-04\n",
      "Epoch 60/100\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 8.9816e-04\n",
      "Epoch 61/100\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 9.1025e-04\n",
      "Epoch 62/100\n",
      "76/76 [==============================] - 13s 168ms/step - loss: 9.0524e-04\n",
      "Epoch 63/100\n",
      "76/76 [==============================] - 13s 170ms/step - loss: 8.3450e-04\n",
      "Epoch 64/100\n",
      "76/76 [==============================] - 13s 168ms/step - loss: 8.8143e-04\n",
      "Epoch 65/100\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 7.8554e-04\n",
      "Epoch 66/100\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 7.8443e-04\n",
      "Epoch 67/100\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 8.8966e-04\n",
      "Epoch 68/100\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 8.2359e-04\n",
      "Epoch 69/100\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 8.9919e-04\n",
      "Epoch 70/100\n",
      "76/76 [==============================] - 13s 168ms/step - loss: 8.7777e-04\n",
      "Epoch 71/100\n",
      "76/76 [==============================] - 12s 161ms/step - loss: 8.2028e-04\n",
      "Epoch 72/100\n",
      "76/76 [==============================] - 13s 171ms/step - loss: 7.8659e-04\n",
      "Epoch 73/100\n",
      "76/76 [==============================] - 13s 167ms/step - loss: 7.9970e-04\n",
      "Epoch 74/100\n",
      "76/76 [==============================] - 12s 161ms/step - loss: 8.3219e-04\n",
      "Epoch 75/100\n",
      "76/76 [==============================] - 12s 161ms/step - loss: 8.5892e-04\n",
      "Epoch 76/100\n",
      "76/76 [==============================] - 16s 206ms/step - loss: 8.4446e-04\n",
      "Epoch 77/100\n",
      "76/76 [==============================] - 14s 180ms/step - loss: 8.4756e-04\n",
      "Epoch 78/100\n",
      "76/76 [==============================] - 13s 168ms/step - loss: 8.4246e-04\n",
      "Epoch 79/100\n",
      "76/76 [==============================] - 12s 160ms/step - loss: 9.1187e-04\n",
      "Epoch 80/100\n",
      "76/76 [==============================] - 12s 160ms/step - loss: 8.4345e-04\n",
      "Epoch 81/100\n",
      "76/76 [==============================] - 12s 161ms/step - loss: 7.9139e-04\n",
      "Epoch 82/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 8.1060e-04\n",
      "Epoch 83/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 8.1719e-04\n",
      "Epoch 84/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.7369e-04\n",
      "Epoch 85/100\n",
      "76/76 [==============================] - 12s 159ms/step - loss: 8.3031e-04\n",
      "Epoch 86/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 8.6135e-04\n",
      "Epoch 87/100\n",
      "76/76 [==============================] - 12s 160ms/step - loss: 7.3988e-04\n",
      "Epoch 88/100\n",
      "76/76 [==============================] - 12s 161ms/step - loss: 8.7139e-04\n",
      "Epoch 89/100\n",
      "76/76 [==============================] - 12s 159ms/step - loss: 8.2764e-04\n",
      "Epoch 90/100\n",
      "76/76 [==============================] - 12s 159ms/step - loss: 6.8299e-04\n",
      "Epoch 91/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.8355e-04\n",
      "Epoch 92/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.9699e-04\n",
      "Epoch 93/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.9382e-04\n",
      "Epoch 94/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.0925e-04\n",
      "Epoch 95/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.5335e-04\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 12s 159ms/step - loss: 6.9077e-04\n",
      "Epoch 97/100\n",
      "76/76 [==============================] - 14s 180ms/step - loss: 7.0016e-04\n",
      "Epoch 98/100\n",
      "76/76 [==============================] - 13s 167ms/step - loss: 7.8554e-04\n",
      "Epoch 99/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.4444e-04\n",
      "Epoch 100/100\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 7.8024e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5990416430>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the Intel Stock Price Test set for July 2019 using pandas and make it a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>50.520000</td>\n",
       "      <td>51.889999</td>\n",
       "      <td>49.470001</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>48.059563</td>\n",
       "      <td>34020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>49.090000</td>\n",
       "      <td>49.360001</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>48.680000</td>\n",
       "      <td>47.263428</td>\n",
       "      <td>27881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>47.759998</td>\n",
       "      <td>47.959999</td>\n",
       "      <td>46.570000</td>\n",
       "      <td>46.970001</td>\n",
       "      <td>45.603188</td>\n",
       "      <td>38936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>47.150002</td>\n",
       "      <td>47.560001</td>\n",
       "      <td>46.770000</td>\n",
       "      <td>46.959999</td>\n",
       "      <td>45.901310</td>\n",
       "      <td>26119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>46.299999</td>\n",
       "      <td>46.880001</td>\n",
       "      <td>45.970001</td>\n",
       "      <td>46.730000</td>\n",
       "      <td>45.676498</td>\n",
       "      <td>29440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>46.160000</td>\n",
       "      <td>47.369999</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>47.169998</td>\n",
       "      <td>46.106575</td>\n",
       "      <td>30643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>46.939999</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>45.779999</td>\n",
       "      <td>45.980000</td>\n",
       "      <td>44.943405</td>\n",
       "      <td>24975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>45.759998</td>\n",
       "      <td>46.070000</td>\n",
       "      <td>45.439999</td>\n",
       "      <td>45.599998</td>\n",
       "      <td>44.571964</td>\n",
       "      <td>18490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>45.490002</td>\n",
       "      <td>47.049999</td>\n",
       "      <td>45.360001</td>\n",
       "      <td>46.840000</td>\n",
       "      <td>45.784016</td>\n",
       "      <td>28959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>46.060001</td>\n",
       "      <td>46.630001</td>\n",
       "      <td>45.650002</td>\n",
       "      <td>45.869999</td>\n",
       "      <td>44.835884</td>\n",
       "      <td>25650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>46.099998</td>\n",
       "      <td>46.180000</td>\n",
       "      <td>45.439999</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>44.669716</td>\n",
       "      <td>21896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>46.340000</td>\n",
       "      <td>46.680000</td>\n",
       "      <td>46.060001</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>45.451683</td>\n",
       "      <td>22701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>47.459999</td>\n",
       "      <td>47.599998</td>\n",
       "      <td>47.040001</td>\n",
       "      <td>47.230000</td>\n",
       "      <td>46.165222</td>\n",
       "      <td>21403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>47.029999</td>\n",
       "      <td>47.119999</td>\n",
       "      <td>46.459999</td>\n",
       "      <td>46.599998</td>\n",
       "      <td>45.549427</td>\n",
       "      <td>23113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-08-21</td>\n",
       "      <td>47.110001</td>\n",
       "      <td>47.290001</td>\n",
       "      <td>46.889999</td>\n",
       "      <td>47.150002</td>\n",
       "      <td>46.087029</td>\n",
       "      <td>15915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>47.279999</td>\n",
       "      <td>47.430000</td>\n",
       "      <td>46.689999</td>\n",
       "      <td>46.779999</td>\n",
       "      <td>45.725368</td>\n",
       "      <td>19783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-08-23</td>\n",
       "      <td>46.349998</td>\n",
       "      <td>46.630001</td>\n",
       "      <td>44.799999</td>\n",
       "      <td>44.959999</td>\n",
       "      <td>43.946400</td>\n",
       "      <td>32814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>45.820000</td>\n",
       "      <td>45.820000</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>45.560001</td>\n",
       "      <td>44.532875</td>\n",
       "      <td>22080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>45.869999</td>\n",
       "      <td>46.119999</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>44.757687</td>\n",
       "      <td>16925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>45.369999</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>44.757687</td>\n",
       "      <td>14888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>46.459999</td>\n",
       "      <td>47.220001</td>\n",
       "      <td>46.400002</td>\n",
       "      <td>46.869999</td>\n",
       "      <td>45.813339</td>\n",
       "      <td>17803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>47.240002</td>\n",
       "      <td>47.790001</td>\n",
       "      <td>47.160000</td>\n",
       "      <td>47.410000</td>\n",
       "      <td>46.341164</td>\n",
       "      <td>16922600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2019-08-01  50.520000  51.889999  49.470001  49.500000  48.059563   \n",
       "1   2019-08-02  49.090000  49.360001  48.500000  48.680000  47.263428   \n",
       "2   2019-08-05  47.759998  47.959999  46.570000  46.970001  45.603188   \n",
       "3   2019-08-06  47.150002  47.560001  46.770000  46.959999  45.901310   \n",
       "4   2019-08-07  46.299999  46.880001  45.970001  46.730000  45.676498   \n",
       "5   2019-08-08  46.160000  47.369999  45.910000  47.169998  46.106575   \n",
       "6   2019-08-09  46.939999  47.000000  45.779999  45.980000  44.943405   \n",
       "7   2019-08-12  45.759998  46.070000  45.439999  45.599998  44.571964   \n",
       "8   2019-08-13  45.490002  47.049999  45.360001  46.840000  45.784016   \n",
       "9   2019-08-14  46.060001  46.630001  45.650002  45.869999  44.835884   \n",
       "10  2019-08-15  46.099998  46.180000  45.439999  45.700001  44.669716   \n",
       "11  2019-08-16  46.340000  46.680000  46.060001  46.500000  45.451683   \n",
       "12  2019-08-19  47.459999  47.599998  47.040001  47.230000  46.165222   \n",
       "13  2019-08-20  47.029999  47.119999  46.459999  46.599998  45.549427   \n",
       "14  2019-08-21  47.110001  47.290001  46.889999  47.150002  46.087029   \n",
       "15  2019-08-22  47.279999  47.430000  46.689999  46.779999  45.725368   \n",
       "16  2019-08-23  46.349998  46.630001  44.799999  44.959999  43.946400   \n",
       "17  2019-08-26  45.820000  45.820000  45.250000  45.560001  44.532875   \n",
       "18  2019-08-27  45.869999  46.119999  45.500000  45.790001  44.757687   \n",
       "19  2019-08-28  45.700001  45.910000  45.369999  45.790001  44.757687   \n",
       "20  2019-08-29  46.459999  47.220001  46.400002  46.869999  45.813339   \n",
       "21  2019-08-30  47.240002  47.790001  47.160000  47.410000  46.341164   \n",
       "\n",
       "      Volume  \n",
       "0   34020800  \n",
       "1   27881600  \n",
       "2   38936600  \n",
       "3   26119600  \n",
       "4   29440400  \n",
       "5   30643700  \n",
       "6   24975500  \n",
       "7   18490300  \n",
       "8   28959500  \n",
       "9   25650200  \n",
       "10  21896000  \n",
       "11  22701800  \n",
       "12  21403600  \n",
       "13  23113300  \n",
       "14  15915000  \n",
       "15  19783000  \n",
       "16  32814000  \n",
       "17  22080500  \n",
       "18  16925500  \n",
       "19  14888800  \n",
       "20  17803800  \n",
       "21  16922600  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price for July 1st 2019 - \n",
    "# July 31st 2019\n",
    "\n",
    "dataset_test = pd.read_csv('Intel_Stock_Price_Test.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50.52    ],\n",
       "       [49.09    ],\n",
       "       [47.759998],\n",
       "       [47.150002],\n",
       "       [46.299999],\n",
       "       [46.16    ],\n",
       "       [46.939999],\n",
       "       [45.759998],\n",
       "       [45.490002],\n",
       "       [46.060001],\n",
       "       [46.099998],\n",
       "       [46.34    ],\n",
       "       [47.459999],\n",
       "       [47.029999],\n",
       "       [47.110001],\n",
       "       [47.279999],\n",
       "       [46.349998],\n",
       "       [45.82    ],\n",
       "       [45.869999],\n",
       "       [45.700001],\n",
       "       [46.459999],\n",
       "       [47.240002]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in July 2019, we need the values in the last 90 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "\n",
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2019, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 90 from inputs are from training set; start \n",
    "# from 90 and get the extra 20, i.e. up to 112\n",
    "for i in range(90, 112): \n",
    "    X_test.append(inputs[i-90:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # not 3D structure yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to inverse the scaling to get meaningful predicted stock price # outputs\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Intel Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Intel Stock Price')\n",
    "\n",
    "plt.title('Intel Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Intel Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the trend of the stock for the month of July 2019. \n",
    "\n",
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown.\n",
    "\n",
    "Then consider dividing this RMSE by the range of the Intel Stock Price values of July 2019 to get a relative error, as opposed to an absolute error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.086899795564367"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:22,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
